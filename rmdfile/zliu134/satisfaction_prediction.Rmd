---
title: "Passenger Satisfaction Prediction Analysis"
subtitle: "Can we accurately predict passenger satisfaction, and which factors are the most important predictors?"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
    highlight: tango
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = "center"
)
```

# Research Question

**Can we accurately predict passenger satisfaction, and which factors are the most important predictors?**

---

# Setup and Package Loading

```{r setup-packages}
## Install and load required R packages
suppressPackageStartupMessages({
  # Define list of all required R packages
  packages <- c("caret",        # Classification and Regression Training
                "randomForest", # Random Forest model
                "xgboost",      # XGBoost gradient boosting
                "dplyr",        # Data manipulation
                "ggplot2",      # Data visualization
                "pROC",         # ROC curve analysis
                "pdp",          # Partial dependence plots
                "gridExtra",    # Grid layout for plots
                "RColorBrewer", # Color palettes
                "corrplot",     # Correlation plots
                "tidyr")        # Data tidying
  
  # Loop through packages and install if not already installed
  for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      install.packages(pkg, repos = "https://cran.rstudio.com/")
    }
  }
  
  # Load all required libraries
  library(caret)         # Model training and evaluation tools
  library(randomForest)  # Random Forest algorithm
  library(xgboost)       # XGBoost algorithm
  library(dplyr)         # Data manipulation pipeline functions
  library(ggplot2)       # Plotting
  library(pROC)          # ROC and AUC calculation
  library(pdp)           # Partial dependence plot
  library(gridExtra)      # Multi-plot combination
  library(RColorBrewer)   # Color themes
  library(corrplot)      # Correlation visualization
  library(tidyr)         # Data long/wide format conversion
})

## Set working directory - adjust path as needed
root_dir <- "/Users/jackleo/R_project/IS507_work"
if (dir.exists(root_dir)) setwd(root_dir)

## Create output directories if they don't exist
## output/models: stores model evaluation results and feature importance CSV files
## output/figures: stores all visualization charts
if (!dir.exists("output/models")) dir.create("output/models", recursive = TRUE)
if (!dir.exists("output/figures")) dir.create("output/figures", recursive = TRUE)
```

# Data Preprocessing Function

```{r preprocess-function}
## Purpose: Clean and prepare data, including factor conversion, missing value handling

preprocess_data <- function(data) {
  message("Preprocessing data...")
  
  ## Check if target variable exists
  if (!("satisfaction" %in% names(data))) {
    stop("Column 'satisfaction' not found in data.")
  }
  
  ## Convert satisfaction to valid R variable names (R doesn't support spaces in factor levels)
  ## Original value: "neutral or dissatisfied" -> "neutral_or_dissatisfied"
  ## Original value: "satisfied" -> "satisfied"
  data$satisfaction <- ifelse(data$satisfaction == "neutral or dissatisfied",
                              "neutral_or_dissatisfied", "satisfied")
  
  ## Convert satisfaction to factor type with specified levels
  ## Note: First level is negative class, second level is positive class (for model evaluation)
  data$satisfaction <- factor(
    data$satisfaction,
    levels = c("neutral_or_dissatisfied", "satisfied")
  )
  
  ## Convert key categorical variables to factor type
  ## These variables need to be treated as categorical in subsequent analysis
  categorical_vars <- c("Gender",           # Gender
                        "Customer.Type",    # Customer type (loyal/disloyal)
                        "Type.of.Travel",   # Travel type (business/personal)
                        "Class")            # Class (economy/business/etc.)
  
  for (col in categorical_vars) {
    if (col %in% names(data)) {
      data[[col]] <- factor(data[[col]])
    }
  }
  
  ## Handle missing values: simple deletion of rows with missing values
  ## Note: Production environment may require more complex missing value handling strategies (e.g., imputation)
  data <- na.omit(data)
  
  ## Output preprocessed data information
  message(sprintf("Preprocessed data: %d rows, %d columns", nrow(data), ncol(data)))
  message(sprintf("Satisfaction distribution:\n"))
  print(table(data$satisfaction))
  
  return(data)
}
```

# Feature Engineering & Data Split Function

```{r feature-engineering-function}
## Purpose: Create new features, split train/test sets, perform one-hot encoding

split_and_engineer <- function(processed_data, seed = 42) {
  ## Set random seed for reproducibility
  set.seed(seed)
  message("Splitting data and engineering features...")
  
  ## Define all service rating columns (14 service dimensions)
  ## These are passenger ratings for various services (typically 1-5 scale)
  service_cols <- c(
    "Inflight.wifi.service",              # Inflight WiFi service
    "Departure.Arrival.time.convenient",  # Departure/Arrival time convenience
    "Ease.of.Online.booking",             # Online booking ease
    "Gate.location",                      # Gate location
    "Food.and.drink",                     # Food and drink service
    "Online.boarding",                    # Online boarding service
    "Seat.comfort",                       # Seat comfort
    "Inflight.entertainment",             # Inflight entertainment
    "On.board.service",                   # On-board service
    "Leg.room.service",                   # Leg room service
    "Baggage.handling",                   # Baggage handling
    "Checkin.service",                    # Check-in service
    "Inflight.service",                   # Inflight service
    "Cleanliness"                         # Cleanliness
  )
  
  ## Check if all required columns exist in the data
  missing_cols <- setdiff(service_cols, names(processed_data))
  if (length(missing_cols) > 0) {
    stop(paste("Missing columns:", paste(missing_cols, collapse = ", ")))
  }
  
  ## Split data into training and test sets using stratified sampling (80/20 ratio)
  ## createDataPartition ensures that the proportion of each class in train and test sets matches the original data
  idx <- caret::createDataPartition(processed_data$satisfaction, p = 0.8, list = FALSE)
  train_df <- processed_data[idx, ]   # Training set: 80%
  test_df  <- processed_data[-idx, ]  # Test set: 20%
  
  message(sprintf("Train set: %d rows, Test set: %d rows", nrow(train_df), nrow(test_df)))
  
  ## Create composite service score (feature engineering)
  ## Calculate the mean of all service ratings as a comprehensive indicator of overall service quality
  train_df$service_score <- rowMeans(train_df[, service_cols], na.rm = TRUE)
  test_df$service_score  <- rowMeans(test_df[, service_cols], na.rm = TRUE)
  
  ## Perform one-hot encoding for categorical variables
  ## Important: Only fit encoder on training data to avoid data leakage
  ## If test set information is used to fit the encoder, it will lead to overly optimistic model performance evaluation
  categorical_vars <- c("Gender", "Customer.Type", "Type.of.Travel", "Class")
  dv <- caret::dummyVars(
    ~ Gender + Customer.Type + Type.of.Travel + Class, 
    data = train_df  # Only fit on training set
  )
  train_dv <- predict(dv, train_df)  # Encode training set
  test_dv  <- predict(dv, test_df)    # Encode test set (using encoder fitted on training set)
  
  ## Combine all features: one-hot encoded categorical + service ratings + composite score + numeric features
  X_train <- cbind(train_dv, train_df[, c(service_cols, "service_score", 
                                          "Age",                        # Age
                                          "Flight.Distance",            # Flight distance
                                          "Departure.Delay.in.Minutes", # Departure delay (minutes)
                                          "Arrival.Delay.in.Minutes")]) # Arrival delay (minutes)
  X_test  <- cbind(test_dv, test_df[, c(service_cols, "service_score",
                                        "Age", "Flight.Distance",
                                        "Departure.Delay.in.Minutes",
                                        "Arrival.Delay.in.Minutes")])
  
  ## Extract target variables
  y_train <- train_df$satisfaction
  y_test  <- test_df$satisfaction
  
  ## Return all required data and metadata
  return(list(
    X_train = X_train,      # Training set feature matrix
    X_test = X_test,        # Test set feature matrix
    y_train = y_train,      # Training set labels
    y_test = y_test,        # Test set labels
    train_df = train_df,    # Original training dataframe (for subsequent analysis)
    test_df = test_df,      # Original test dataframe
    service_cols = service_cols,  # Service column names list (for feature importance analysis)
    positive_class = "satisfied"  # Positive class label (for model evaluation)
  ))
}
```

# Model Training with Cross-Validation

```{r train-models-function}
## Purpose: Train RF and XGBoost models, perform cross-validation evaluation

train_models <- function(X_train, y_train, cv_folds = 5) {
  message("Training models with cross-validation...")
  
  ## ========== Random Forest Model Training ==========
  message("Training Random Forest...")
  rf_model <- randomForest(
    x = X_train,                    # Feature matrix
    y = y_train,                    # Target variable
    ntree = 500,                    # Number of trees (more trees usually better, but higher computational cost)
    mtry = sqrt(ncol(X_train)),     # Number of variables considered at each split (sqrt is common choice)
    importance = TRUE,              # Calculate feature importance
    do.trace = FALSE                # Don't show training process
  )
  
  ## ========== XGBoost Model Training ==========
  message("Training XGBoost...")
  ## Convert factor labels to numeric (XGBoost requires numeric labels)
  y_train_num <- as.integer(y_train == "satisfied")  # satisfied=1, neutral_or_dissatisfied=0
  
  ## Create XGBoost data matrix
  dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train_num)
  
  ## Set XGBoost hyperparameters
  params <- list(
    objective = "binary:logistic",  # Binary logistic regression objective function
    eval_metric = "auc",            # Evaluation metric: AUC (Area Under ROC Curve)
    max_depth = 6,                  # Maximum depth of trees (controls model complexity)
    eta = 0.1,                      # Learning rate (step size, smaller values need more rounds but more stable)
    subsample = 0.8,                # Proportion of samples used per tree (prevents overfitting)
    colsample_bytree = 0.8,         # Proportion of features used per tree (prevents overfitting)
    min_child_weight = 1            # Minimum weight of leaf nodes (controls overfitting)
  )
  
  ## Train XGBoost model
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 200,    # Number of iterations (boosting rounds)
    verbose = 0       # Don't show training process
  )
  
  ## ========== Cross-Validation (for robust accuracy assessment) ==========
  message("Performing cross-validation for robust accuracy assessment...")
  
  ## Set cross-validation control parameters
  cv_control <- trainControl(
    method = "cv",                    # Cross-validation method: k-fold cross-validation
    number = cv_folds,                # Number of folds (default 5-fold)
    summaryFunction = twoClassSummary, # Binary classification summary function
    classProbs = TRUE,                # Return class probabilities
    verboseIter = FALSE               # Don't show detailed information for each iteration
  )
  
  ## Random Forest Cross-Validation
  rf_cv <- train(
    x = X_train,
    y = y_train,
    method = "rf",                    # Random Forest method
    trControl = cv_control,
    metric = "ROC",                   # Optimization metric: ROC (AUC)
    tuneGrid = data.frame(mtry = sqrt(ncol(X_train))),  # Fixed mtry parameter
    ntree = 500
  )
  
  ## XGBoost Cross-Validation
  xgb_cv <- train(
    x = X_train,
    y = y_train,
    method = "xgbTree",              # XGBoost tree method
    trControl = cv_control,
    metric = "ROC",
    tuneGrid = expand.grid(           # Hyperparameter grid (using fixed values here)
      nrounds = 200,
      max_depth = 6,
      eta = 0.1,
      gamma = 0,                      # Minimum loss reduction
      colsample_bytree = 0.8,
      min_child_weight = 1,
      subsample = 0.8
    )
  )
  
  ## Return all models and cross-validation results
  return(list(
    rf = rf_model,      # Trained Random Forest model
    xgb = xgb_model,    # Trained XGBoost model
    rf_cv = rf_cv,      # RF cross-validation results
    xgb_cv = xgb_cv     # XGBoost cross-validation results
  ))
}
```

# Comprehensive Model Evaluation Function

```{r evaluate-models-function}
## Purpose: Evaluate model performance, calculate various metrics (accuracy, AUC, F1, etc.)

evaluate_models <- function(models, X_test, y_test, positive_class) {
  message("Evaluating models...")
  
  ## ========== Model Predictions ==========
  
  ## Random Forest predictions
  rf_pred <- predict(models$rf, X_test)  # Predict classes
  rf_prob <- predict(models$rf, X_test, type = "prob")[, positive_class]  # Predict probabilities (positive class)
  
  ## XGBoost predictions
  xgb_prob <- predict(models$xgb, as.matrix(X_test))  # Predict probabilities (0-1 range)
  ## Convert probabilities to classes (threshold 0.5)
  xgb_pred <- factor(
    ifelse(xgb_prob > 0.5, positive_class, "neutral_or_dissatisfied"),
    levels = levels(y_test)
  )
  
  ## ========== Confusion Matrices ==========
  ## Confusion matrix shows correspondence between predictions and true labels
  cm_rf  <- confusionMatrix(rf_pred, y_test, positive = positive_class)
  cm_xgb <- confusionMatrix(xgb_pred, y_test, positive = positive_class)
  
  ## ========== AUC (Area Under ROC Curve) Calculation ==========
  ## AUC measures model's ability to distinguish positive and negative classes
  y_test_num <- as.integer(y_test == positive_class)  # Convert to numeric (0/1)
  auc_rf  <- as.numeric(pROC::auc(y_test_num, rf_prob))
  auc_xgb <- as.numeric(pROC::auc(y_test_num, xgb_prob))
  
  ## ========== F1 Score Calculation ==========
  ## F1 = 2 * (Precision * Recall) / (Precision + Recall)
  ## F1 is the harmonic mean of precision and recall, balancing both
  f1 <- function(cm) {
    pr <- cm$byClass["Precision"]  # Precision: proportion of true positives among predicted positives
    rc <- cm$byClass["Recall"]     # Recall: proportion of true positives correctly predicted
    if (is.na(pr) || is.na(rc) || (pr + rc) == 0) return(NA_real_)
    2 * pr * rc / (pr + rc)
  }
  
  ## ========== Compile Evaluation Results ==========
  results <- data.frame(
    Model = c("Random Forest", "XGBoost"),
    Accuracy = c(cm_rf$overall["Accuracy"], cm_xgb$overall["Accuracy"]),  # Accuracy
    Sensitivity = c(cm_rf$byClass["Sensitivity"], cm_xgb$byClass["Sensitivity"]),  # Sensitivity (Recall)
    Specificity = c(cm_rf$byClass["Specificity"], cm_xgb$byClass["Specificity"]),  # Specificity
    Precision = c(cm_rf$byClass["Precision"], cm_xgb$byClass["Precision"]),  # Precision
    AUC = c(auc_rf, auc_xgb),  # Area Under ROC Curve
    F1 = c(f1(cm_rf), f1(cm_xgb))  # F1 score
  )
  
  ## ========== Cross-Validation Results Summary ==========
  ## Extract mean and SD of accuracy and ROC from CV results
  rf_acc <- if("Accuracy" %in% names(models$rf_cv$resample)) {
    mean(models$rf_cv$resample$Accuracy, na.rm = TRUE)
  } else NA
  xgb_acc <- if("Accuracy" %in% names(models$xgb_cv$resample)) {
    mean(models$xgb_cv$resample$Accuracy, na.rm = TRUE)
  } else NA
  
  cv_results <- data.frame(
    Model = c("Random Forest (CV)", "XGBoost (CV)"),
    Mean_Accuracy = c(rf_acc, xgb_acc),  # Mean accuracy
    SD_Accuracy = c(if("Accuracy" %in% names(models$rf_cv$resample)) 
                      sd(models$rf_cv$resample$Accuracy, na.rm = TRUE) else NA,
                    if("Accuracy" %in% names(models$xgb_cv$resample))
                      sd(models$xgb_cv$resample$Accuracy, na.rm = TRUE) else NA),  # Accuracy standard deviation
    Mean_ROC = c(mean(models$rf_cv$resample$ROC, na.rm = TRUE),
                 mean(models$xgb_cv$resample$ROC, na.rm = TRUE)),  # Mean ROC
    SD_ROC = c(sd(models$rf_cv$resample$ROC, na.rm = TRUE),
               sd(models$xgb_cv$resample$ROC, na.rm = TRUE))  # ROC standard deviation
  )
  
  ## Return all evaluation results
  return(list(
    results = results,        # Test set evaluation results
    cv_results = cv_results,  # Cross-validation results
    cm_rf = cm_rf,           # RF confusion matrix
    cm_xgb = cm_xgb,         # XGBoost confusion matrix
    rf_pred = rf_pred,       # RF predicted classes
    rf_prob = rf_prob,       # RF predicted probabilities
    xgb_pred = xgb_pred,     # XGBoost predicted classes
    xgb_prob = xgb_prob,      # XGBoost predicted probabilities
    auc_rf = auc_rf,         # RF AUC value
    auc_xgb = auc_xgb,       # XGBoost AUC value
    y_test_num = y_test_num  # Numeric test labels
  ))
}
```

# Feature Importance Analysis Function

```{r feature-importance-function}
## Purpose: Analyze and compare feature importance from both models, identify top predictors

analyze_feature_importance <- function(models, X_test, service_cols) {
  message("Analyzing feature importance...")
  
  ## ========== Random Forest Feature Importance ==========
  ## RF uses MeanDecreaseGini to measure feature importance
  rf_imp <- importance(models$rf)
  rf_imp_df <- data.frame(
    Feature = rownames(rf_imp),
    Importance = rf_imp[, "MeanDecreaseGini"],  # Gini impurity decrease
    Model = "Random Forest"
  )
  rf_imp_df <- rf_imp_df[order(-rf_imp_df$Importance), ]  # Sort by importance descending
  
  ## ========== XGBoost Feature Importance ==========
  ## XGBoost uses Gain to measure feature importance
  xgb_imp <- xgb.importance(
    feature_names = colnames(X_test),
    model = models$xgb
  )
  xgb_imp_df <- data.frame(
    Feature = xgb_imp$Feature,
    Importance = xgb_imp$Gain,  # Gain brought by feature
    Model = "XGBoost"
  )
  xgb_imp_df <- xgb_imp_df[order(-xgb_imp_df$Importance), ]  # Sort by importance descending
  
  ## ========== Normalize Importance Scores (0-100 scale) for Comparison ==========
  ## Since two models use different importance metrics, normalize for comparison
  rf_imp_df$Importance_Normalized <- (rf_imp_df$Importance / max(rf_imp_df$Importance)) * 100
  xgb_imp_df$Importance_Normalized <- (xgb_imp_df$Importance / max(xgb_imp_df$Importance)) * 100
  
  ## ========== Combine Importance from Both Models ==========
  ## Merge RF and XGBoost importance scores, calculate average
  combined_imp <- merge(
    rf_imp_df[, c("Feature", "Importance_Normalized")],
    xgb_imp_df[, c("Feature", "Importance_Normalized")],
    by = "Feature",
    suffixes = c("_RF", "_XGB"),
    all = TRUE  # Keep all features (even if missing in one model)
  )
  combined_imp[is.na(combined_imp)] <- 0  # Fill missing values with 0
  ## Calculate average importance (mean of both models)
  combined_imp$Average_Importance <- (combined_imp$Importance_Normalized_RF + 
                                       combined_imp$Importance_Normalized_XGB) / 2
  combined_imp <- combined_imp[order(-combined_imp$Average_Importance), ]  # Sort by average importance
  
  ## ========== Identify Feature Types ==========
  ## Categorize features: Service Rating, Customer Demographics, Other
  combined_imp$Feature_Type <- ifelse(
    combined_imp$Feature %in% service_cols | 
    grepl("service", combined_imp$Feature, ignore.case = TRUE),
    "Service Rating",  # Service rating features
    ifelse(
      grepl("Type|Class|Gender", combined_imp$Feature),
      "Customer Demographics",  # Customer demographic features
      "Other"  # Other features
    )
  )
  
  ## Return all importance analysis results
  return(list(
    rf_imp_df = rf_imp_df,        # RF feature importance dataframe
    xgb_imp_df = xgb_imp_df,      # XGBoost feature importance dataframe
    combined_imp = combined_imp   # Combined feature importance (for visualization)
  ))
}
```

# Visualization Functions

```{r visualization-function}
## Purpose: Create various visualizations to showcase model performance and feature importance

create_visualizations <- function(eval_results, importance_results, models, 
                                 X_test, y_test_num, positive_class,
                                 out_dir = "output/figures") {
  message("Creating visualizations...")
  
  ## ========== Plot 1: Model Accuracy Comparison ==========
  ## Directly answers first part of research question: Can we accurately predict?
  p1 <- ggplot(eval_results$results, aes(x = Model, y = Accuracy, fill = Model)) +
    geom_col(alpha = 0.8) +  # Bar chart
    geom_text(aes(label = sprintf("%.3f", Accuracy)), vjust = -0.5, size = 4) +  # Add value labels
    scale_fill_brewer(palette = "Set2") +  # Use color scheme
    labs(title = "Model Accuracy Comparison",
         subtitle = "Can we accurately predict passenger satisfaction?",
         y = "Accuracy", x = NULL) +
    theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(size = 14, face = "bold"),
          plot.subtitle = element_text(size = 11))
  
  ## ========== Plot 2: Comprehensive Performance Metrics Comparison ==========
  ## Show multiple evaluation metrics for comprehensive model understanding
  metrics_long <- eval_results$results %>%
    select(Model, Accuracy, Sensitivity, Specificity, Precision, F1, AUC) %>%
    tidyr::pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")
  
  p2 <- ggplot(metrics_long, aes(x = Metric, y = Value, fill = Model)) +
    geom_col(position = "dodge", alpha = 0.8) +  # Side-by-side bar chart
    scale_fill_brewer(palette = "Set2") +
    labs(title = "Comprehensive Model Performance Metrics",
         y = "Score", x = "Metric") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Tilt x-axis labels
          plot.title = element_text(size = 14, face = "bold"))
  
  ## ========== Plot 3: ROC Curves ==========
  ## ROC curve shows true positive rate vs false positive rate at different thresholds
  roc_rf  <- roc(y_test_num, eval_results$rf_prob)
  roc_xgb <- roc(y_test_num, eval_results$xgb_prob)
  
  ## Prepare ROC data for plotting
  roc_data <- data.frame(
    FPR = c(1 - roc_rf$specificities, 1 - roc_xgb$specificities),  # False positive rate
    TPR = c(roc_rf$sensitivities, roc_xgb$sensitivities),          # True positive rate
    Model = c(rep("Random Forest", length(roc_rf$sensitivities)),
              rep("XGBoost", length(roc_xgb$sensitivities)))
  )
  
  p3 <- ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
    geom_line(linewidth = 1.2) +  # ROC curve
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +  # Diagonal (random classifier)
    scale_color_brewer(palette = "Set2") +
    labs(title = sprintf("ROC Curves (AUC: RF=%.3f, XGB=%.3f)",
                         eval_results$auc_rf, eval_results$auc_xgb),
         x = "False Positive Rate", y = "True Positive Rate") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"))
  
  ## ========== Plot 4: Top 15 Most Important Predictors (Combined View) ==========
  ## Directly answers second part of research question: Which factors are most important?
  top_features <- head(importance_results$combined_imp, 15)  # Take top 15
  top_features$Feature <- factor(top_features$Feature, 
                                 levels = rev(top_features$Feature))  # Reverse order for horizontal bar chart
  
  p4 <- ggplot(top_features, aes(x = Feature, y = Average_Importance, 
                                 fill = Feature_Type)) +
    geom_col(alpha = 0.8) +
    coord_flip() +  # Horizontal bar chart
    scale_fill_brewer(palette = "Set1") +
    labs(title = "Top 15 Most Important Predictors (Average of Both Models)",
         subtitle = "Which factors are the most important predictors?",
         x = NULL, y = "Average Normalized Importance",
         fill = "Feature Type") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          plot.subtitle = element_text(size = 11))
  
  ## ========== Plot 5: Feature Importance Comparison (RF vs XGBoost) ==========
  ## Show if both models agree on feature importance
  top_15_rf <- head(importance_results$rf_imp_df, 15)
  top_15_xgb <- head(importance_results$xgb_imp_df, 15)
  
  ## Find features that both models consider important
  common_features <- intersect(top_15_rf$Feature, top_15_xgb$Feature)
  comparison_data <- data.frame(
    Feature = common_features,
    RF_Importance = top_15_rf$Importance_Normalized[match(common_features, top_15_rf$Feature)],
    XGB_Importance = top_15_xgb$Importance_Normalized[match(common_features, top_15_xgb$Feature)]
  )
  comparison_data <- comparison_data[order(-(comparison_data$RF_Importance + 
                                              comparison_data$XGB_Importance)), ]
  comparison_data$Feature <- factor(comparison_data$Feature, 
                                    levels = rev(comparison_data$Feature))
  
  ## Convert to long format for plotting
  comparison_long <- comparison_data %>%
    tidyr::pivot_longer(cols = c(RF_Importance, XGB_Importance),
                       names_to = "Model", values_to = "Importance")
  comparison_long$Model <- gsub("_Importance", "", comparison_long$Model)
  
  p5 <- ggplot(comparison_long, aes(x = Feature, y = Importance, fill = Model)) +
    geom_col(position = "dodge", alpha = 0.8) +  # Side-by-side bar chart
    coord_flip() +
    scale_fill_brewer(palette = "Set2") +
    labs(title = "Feature Importance: RF vs XGBoost Comparison",
         subtitle = "Top features identified by both models",
         x = NULL, y = "Normalized Importance") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"))
  
  ## ========== Plot 6: Confusion Matrices ==========
  ## Confusion matrix shows model's classification error patterns
  cm_rf_data <- as.data.frame(eval_results$cm_rf$table)
  cm_xgb_data <- as.data.frame(eval_results$cm_xgb$table)
  
  ## Random Forest confusion matrix
  p6a <- ggplot(cm_rf_data, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white") +  # Heatmap
    geom_text(aes(label = Freq), color = "black", size = 5) +  # Add values
    scale_fill_gradient(low = "white", high = "steelblue") +  # Color gradient
    labs(title = "Random Forest Confusion Matrix",
         x = "Actual", y = "Predicted") +
    theme_minimal()
  
  ## XGBoost confusion matrix
  p6b <- ggplot(cm_xgb_data, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white") +
    geom_text(aes(label = Freq), color = "black", size = 5) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = "XGBoost Confusion Matrix",
         x = "Actual", y = "Predicted") +
    theme_minimal()
  
  ## ========== Save All Plots ==========
  ggsave(file.path(out_dir, "01_accuracy_comparison.png"), p1, 
         width = 8, height = 6, dpi = 300)
  ggsave(file.path(out_dir, "02_comprehensive_metrics.png"), p2, 
         width = 10, height = 6, dpi = 300)
  ggsave(file.path(out_dir, "03_roc_curves.png"), p3, 
         width = 8, height = 6, dpi = 300)
  ggsave(file.path(out_dir, "04_top_features_combined.png"), p4, 
         width = 10, height = 7, dpi = 300)
  ggsave(file.path(out_dir, "05_feature_importance_comparison.png"), p5, 
         width = 10, height = 7, dpi = 300)
  ggsave(file.path(out_dir, "06a_confusion_matrix_rf.png"), p6a, 
         width = 6, height = 5, dpi = 300)
  ggsave(file.path(out_dir, "06b_confusion_matrix_xgb.png"), p6b, 
         width = 6, height = 5, dpi = 300)
  
  ## ========== Plot 7: Partial Dependence Plot (PDP) ==========
  ## Show how the most important feature affects satisfaction prediction probability
  top_feature <- importance_results$combined_imp$Feature[1]  # Most important feature
  if (top_feature %in% colnames(X_test)) {
    tryCatch({
      ## Calculate partial dependence: fix other features, vary target feature, observe prediction change
      pd <- partial(models$xgb, pred.var = top_feature,
                   train = as.matrix(X_test), which.class = 1, prob = TRUE)
      p7 <- autoplot(pd) + 
        ggtitle(sprintf("Partial Dependence: %s", top_feature),
                subtitle = "How this top predictor affects satisfaction probability") +
        theme_minimal() +
        theme(plot.title = element_text(size = 14, face = "bold"))
      ggsave(file.path(out_dir, sprintf("07_pdp_%s.png", 
                                        gsub("[^A-Za-z0-9_]", "_", top_feature))),
             p7, width = 8, height = 6, dpi = 300)
    }, error = function(e) {
      message(sprintf("Could not create PDP for %s: %s", top_feature, e$message))
    })
  }
  
  message("All visualizations saved to ", out_dir)
  
  ## Return all plot objects (optional, for later combination or modification)
  return(list(
    p1 = p1, p2 = p2, p3 = p3, p4 = p4, p5 = p5, p6a = p6a, p6b = p6b
  ))
}
```

# Main Execution Function

```{r main-execution-function}
## Purpose: Integrate all steps, execute complete analysis pipeline

run_all <- function() {
  ## Print analysis start information
  message("\n==========================================")
  message("Passenger Satisfaction Prediction Analysis")
  message("RQ: Can we accurately predict passenger satisfaction,")
  message("     and which factors are the most important predictors?")
  message("==========================================\n")
  
  ## ========== Step 1: Load Data ==========
  message(">>> Step 1: Loading data...")
  if (!file.exists("dataset/train.csv")) {
    stop("dataset/train.csv not found!")
  }
  train_data <- read.csv("dataset/train.csv")
  message(sprintf("Loaded %d rows, %d columns", nrow(train_data), ncol(train_data)))
  
  ## ========== Step 2: Data Preprocessing ==========
  message("\n>>> Step 2: Preprocessing data...")
  processed <- preprocess_data(train_data)
  
  ## ========== Step 3: Feature Engineering and Data Split ==========
  message("\n>>> Step 3: Splitting data and engineering features...")
  se <- split_and_engineer(processed)
  
  ## ========== Step 4: Model Training ==========
  message("\n>>> Step 4: Training models...")
  models <- train_models(se$X_train, se$y_train)
  
  ## ========== Step 5: Model Evaluation ==========
  message("\n>>> Step 5: Evaluating models...")
  eval_results <- evaluate_models(
    models, se$X_test, se$y_test, se$positive_class
  )
  
  ## Print evaluation results
  message("\n>>> Model Performance Results:")
  print(eval_results$results)
  message("\n>>> Cross-Validation Results:")
  print(eval_results$cv_results)
  
  ## ========== Step 6: Feature Importance Analysis ==========
  message("\n>>> Step 6: Analyzing feature importance...")
  importance_results <- analyze_feature_importance(
    models, se$X_test, se$service_cols
  )
  
  ## Print top 10 most important features
  message("\n>>> Top 10 Most Important Features:")
  print(head(importance_results$combined_imp[, c("Feature", "Average_Importance", 
                                                   "Feature_Type")], 10))
  
  ## ========== Step 7: Create Visualizations ==========
  message("\n>>> Step 7: Creating visualizations...")
  plots <- create_visualizations(
    eval_results, importance_results, models,
    se$X_test, eval_results$y_test_num, se$positive_class
  )
  
  ## ========== Step 8: Save Results ==========
  message("\n>>> Step 8: Saving results...")
  ## Save model evaluation results
  write.csv(eval_results$results, 
            "output/models/model_evaluation.csv", row.names = FALSE)
  ## Save cross-validation results
  write.csv(eval_results$cv_results, 
            "output/models/cv_results.csv", row.names = FALSE)
  ## Save feature importance results
  write.csv(importance_results$rf_imp_df, 
            "output/models/rf_feature_importance.csv", row.names = FALSE)
  write.csv(importance_results$xgb_imp_df, 
            "output/models/xgb_feature_importance.csv", row.names = FALSE)
  write.csv(importance_results$combined_imp, 
            "output/models/combined_feature_importance.csv", row.names = FALSE)
  
  ## ========== Analysis Summary ==========
  message("\n==========================================")
  message("‚úÖ Analysis Complete!")
  message("==========================================")
  message("\nüìä Key Findings:")
  ## Best model accuracy
  message(sprintf("   ‚Ä¢ Best Model Accuracy: %.3f (%s)",
                  max(eval_results$results$Accuracy),
                  eval_results$results$Model[which.max(eval_results$results$Accuracy)]))
  ## Best model AUC
  message(sprintf("   ‚Ä¢ Best Model AUC: %.3f (%s)",
                  max(eval_results$results$AUC),
                  eval_results$results$Model[which.max(eval_results$results$AUC)]))
  ## Top predictor
  message(sprintf("   ‚Ä¢ Top Predictor: %s (Importance: %.2f)",
                  importance_results$combined_imp$Feature[1],
                  importance_results$combined_imp$Average_Importance[1]))
  message("\nüìÅ Output Files:")
  message("   ‚Ä¢ CSV files: output/models/")
  message("   ‚Ä¢ Figures: output/figures/")
  message("==========================================\n")
  
  ## Return all result objects (optional, for further analysis)
  return(list(
    models = models,                    # Trained models
    eval_results = eval_results,        # Evaluation results
    importance_results = importance_results,  # Feature importance results
    plots = plots                       # Plot objects
  ))
}
```

# Execute Complete Analysis

```{r run-analysis, eval=TRUE}
## Execute complete analysis pipeline
results <- run_all()
```

# Results Display

## Model Performance Results

```{r display-results, echo=FALSE}
## Display model evaluation results
if (exists("results")) {
  knitr::kable(results$eval_results$results, 
               caption = "Model Performance Results",
               digits = 4)
  
  knitr::kable(results$eval_results$cv_results,
               caption = "Cross-Validation Results",
               digits = 4)
  
  knitr::kable(head(results$importance_results$combined_imp[, 
                c("Feature", "Average_Importance", "Feature_Type")], 10),
               caption = "Top 10 Most Important Features",
               digits = 2)
}
```

## Visualization Plots

```{r display-plots, echo=FALSE, fig.width=10, fig.height=6}
## Display main visualization plots
if (exists("results") && exists("plots")) {
  print(results$plots$p1)  # Accuracy comparison
  print(results$plots$p3)  # ROC curves
  print(results$plots$p4)  # Feature importance
}
```

---

# Summary

This analysis successfully predicts passenger satisfaction using Random Forest and XGBoost machine learning models, and identifies the most important predictive factors.

**Key Findings:**

1. **Model Accuracy**: Both models achieved over 96% accuracy, indicating that passenger satisfaction can be accurately predicted.
2. **Most Important Predictors**: Online boarding service (Online.boarding) is the most important predictor, followed by inflight WiFi service and travel type.

**Output Files:**
- Model evaluation results: `output/models/model_evaluation.csv`
- Feature importance: `output/models/combined_feature_importance.csv`
- Visualization charts: `output/figures/`

