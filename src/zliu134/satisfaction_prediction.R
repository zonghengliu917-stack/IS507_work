## ============================================================================
## ç ”ç©¶é—®é¢˜ (Research Question): 
## æˆ‘ä»¬èƒ½å¦å‡†ç¡®é¢„æµ‹ä¹˜å®¢æ»¡æ„åº¦ï¼Ÿå“ªäº›å› ç´ æ˜¯æœ€é‡è¦çš„é¢„æµ‹å› å­ï¼Ÿ
## Can we accurately predict passenger satisfaction, and which factors are 
## the most important predictors?
## ============================================================================

## ============================================================================
## 0) ç¯å¢ƒè®¾ç½®ä¸åŒ…åŠ è½½ (Setup and Package Loading)
## ============================================================================
## æ­¤éƒ¨åˆ†ç”¨äºå®‰è£…å’ŒåŠ è½½æ‰€éœ€çš„RåŒ…ï¼Œè®¾ç½®å·¥ä½œç›®å½•ï¼Œåˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
## This section installs and loads required R packages, sets working directory,
## and creates output folders

suppressPackageStartupMessages({
  # å®šä¹‰éœ€è¦ä½¿ç”¨çš„æ‰€æœ‰RåŒ…åˆ—è¡¨
  # Define list of all required R packages
  packages <- c("caret",        # åˆ†ç±»å’Œå›å½’è®­ç»ƒ - Classification and Regression Training
                "randomForest", # éšæœºæ£®æ—æ¨¡å‹ - Random Forest model
                "xgboost",      # XGBoostæ¢¯åº¦æå‡æ¨¡å‹ - XGBoost gradient boosting
                "dplyr",        # æ•°æ®æ“ä½œå’Œè½¬æ¢ - Data manipulation
                "ggplot2",      # æ•°æ®å¯è§†åŒ– - Data visualization
                "pROC",         # ROCæ›²çº¿åˆ†æ - ROC curve analysis
                "pdp",          # åä¾èµ–å›¾ - Partial dependence plots
                "gridExtra",    # å›¾å½¢å¸ƒå±€ - Grid layout for plots
                "RColorBrewer", # é¢œè‰²æ–¹æ¡ˆ - Color palettes
                "corrplot",     # ç›¸å…³æ€§å›¾ - Correlation plots
                "tidyr")        # æ•°æ®æ•´ç† - Data tidying
  
  # å¾ªç¯æ£€æŸ¥æ¯ä¸ªåŒ…æ˜¯å¦å·²å®‰è£…ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨å®‰è£…
  # Loop through packages and install if not already installed
  for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
      install.packages(pkg, repos = "https://cran.rstudio.com/")
    }
  }
  
  # åŠ è½½æ‰€æœ‰å¿…éœ€çš„åº“
  # Load all required libraries
  library(caret)         # æä¾›æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å·¥å…·
  library(randomForest)  # éšæœºæ£®æ—ç®—æ³•
  library(xgboost)       # XGBoostç®—æ³•
  library(dplyr)         # æ•°æ®æ“ä½œç®¡é“å‡½æ•°
  library(ggplot2)       # å›¾å½¢ç»˜åˆ¶
  library(pROC)          # ROCå’ŒAUCè®¡ç®—
  library(pdp)           # åä¾èµ–å›¾ç»˜åˆ¶
  library(gridExtra)      # å¤šå›¾ç»„åˆ
  library(RColorBrewer)   # é¢œè‰²ä¸»é¢˜
  library(corrplot)      # ç›¸å…³æ€§å¯è§†åŒ–
  library(tidyr)         # æ•°æ®é•¿å®½æ ¼å¼è½¬æ¢
})

## è®¾ç½®å·¥ä½œç›®å½• - æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
## Set working directory - adjust path as needed
root_dir <- "/Users/jackleo/R_project/IS507_work"
if (dir.exists(root_dir)) setwd(root_dir)

## åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
## Create output directories if they don't exist
## output/zliu134/models: å­˜å‚¨æ¨¡å‹è¯„ä¼°ç»“æœå’Œç‰¹å¾é‡è¦æ€§CSVæ–‡ä»¶
## output/zliu134/figures: å­˜å‚¨æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
if (!dir.exists("output/zliu134/models")) dir.create("output/zliu134/models", recursive = TRUE)
if (!dir.exists("output/zliu134/figures")) dir.create("output/zliu134/figures", recursive = TRUE)

## ============================================================================
## 1) æ•°æ®é¢„å¤„ç†å‡½æ•° (Data Preprocessing Function)
## ============================================================================
## åŠŸèƒ½ï¼šæ¸…æ´—å’Œå‡†å¤‡æ•°æ®ï¼ŒåŒ…æ‹¬å› å­è½¬æ¢ã€ç¼ºå¤±å€¼å¤„ç†ç­‰
## Purpose: Clean and prepare data, including factor conversion, missing value handling

preprocess_data <- function(data) {
  message("Preprocessing data...")
  
  ## æ£€æŸ¥ç›®æ ‡å˜é‡æ˜¯å¦å­˜åœ¨
  ## Check if target variable exists
  if (!("satisfaction" %in% names(data))) {
    stop("Column 'satisfaction' not found in data.")
  }
  
  ## å°†æ»¡æ„åº¦è½¬æ¢ä¸ºæœ‰æ•ˆçš„Rå˜é‡åï¼ˆRä¸æ”¯æŒåŒ…å«ç©ºæ ¼çš„å› å­æ°´å¹³ï¼‰
  ## Convert satisfaction to valid R variable names (R doesn't support spaces in factor levels)
  ## åŸå§‹å€¼ï¼š"neutral or dissatisfied" -> "neutral_or_dissatisfied"
  ## åŸå§‹å€¼ï¼š"satisfied" -> "satisfied"
  data$satisfaction <- ifelse(data$satisfaction == "neutral or dissatisfied",
                              "neutral_or_dissatisfied", "satisfied")
  
  ## å°†æ»¡æ„åº¦è½¬æ¢ä¸ºå› å­ç±»å‹ï¼Œå¹¶æŒ‡å®šæ°´å¹³é¡ºåº
  ## Convert satisfaction to factor type with specified levels
  ## æ³¨æ„ï¼šç¬¬ä¸€ä¸ªæ°´å¹³æ˜¯è´Ÿç±»ï¼Œç¬¬äºŒä¸ªæ°´å¹³æ˜¯æ­£ç±»ï¼ˆç”¨äºæ¨¡å‹è¯„ä¼°ï¼‰
  data$satisfaction <- factor(
    data$satisfaction,
    levels = c("neutral_or_dissatisfied", "satisfied")
  )
  
  ## å°†å…³é”®åˆ†ç±»å˜é‡è½¬æ¢ä¸ºå› å­ç±»å‹
  ## Convert key categorical variables to factor type
  ## è¿™äº›å˜é‡åœ¨åç»­åˆ†æä¸­éœ€è¦ä½œä¸ºåˆ†ç±»å˜é‡å¤„ç†
  categorical_vars <- c("Gender",           # æ€§åˆ«
                        "Customer.Type",    # å®¢æˆ·ç±»å‹ï¼ˆå¿ è¯š/ä¸å¿ è¯šï¼‰
                        "Type.of.Travel",   # æ—…è¡Œç±»å‹ï¼ˆå•†åŠ¡/ä¸ªäººï¼‰
                        "Class")            # èˆ±ä½ç­‰çº§ï¼ˆç»æµ/å•†åŠ¡ç­‰ï¼‰
  
  for (col in categorical_vars) {
    if (col %in% names(data)) {
      data[[col]] <- factor(data[[col]])
    }
  }
  
  ## å¤„ç†ç¼ºå¤±å€¼ï¼šç®€å•åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
  ## Handle missing values: simple deletion of rows with missing values
  ## æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒå¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥ï¼ˆå¦‚æ’è¡¥ï¼‰
  data <- na.omit(data)
  
  ## è¾“å‡ºé¢„å¤„ç†åçš„æ•°æ®ä¿¡æ¯
  ## Output preprocessed data information
  message(sprintf("Preprocessed data: %d rows, %d columns", nrow(data), ncol(data)))
  message(sprintf("Satisfaction distribution:\n"))
  print(table(data$satisfaction))
  
  return(data)
}

## ============================================================================
## 2) ç‰¹å¾å·¥ç¨‹ä¸æ•°æ®åˆ’åˆ†å‡½æ•° (Feature Engineering & Data Split Function)
## ============================================================================
## åŠŸèƒ½ï¼šåˆ›å»ºæ–°ç‰¹å¾ã€åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ã€è¿›è¡Œç‹¬çƒ­ç¼–ç 
## Purpose: Create new features, split train/test sets, perform one-hot encoding

split_and_engineer <- function(processed_data, seed = 42) {
  ## è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
  ## Set random seed for reproducibility
  set.seed(seed)
  message("Splitting data and engineering features...")
  
  ## å®šä¹‰æ‰€æœ‰æœåŠ¡è¯„åˆ†åˆ—ï¼ˆ14ä¸ªæœåŠ¡ç»´åº¦ï¼‰
  ## Define all service rating columns (14 service dimensions)
  ## è¿™äº›æ˜¯ä¹˜å®¢å¯¹å„ç§æœåŠ¡çš„è¯„åˆ†ï¼ˆé€šå¸¸ä¸º1-5åˆ†ï¼‰
  service_cols <- c(
    "Inflight.wifi.service",              # æœºä¸ŠWiFiæœåŠ¡
    "Departure.Arrival.time.convenient",  # å‡ºå‘/åˆ°è¾¾æ—¶é—´ä¾¿åˆ©æ€§
    "Ease.of.Online.booking",             # åœ¨çº¿é¢„è®¢ä¾¿åˆ©æ€§
    "Gate.location",                      # ç™»æœºå£ä½ç½®
    "Food.and.drink",                     # é¤é¥®æœåŠ¡
    "Online.boarding",                    # åœ¨çº¿ç™»æœºæœåŠ¡
    "Seat.comfort",                       # åº§ä½èˆ’é€‚åº¦
    "Inflight.entertainment",             # æœºä¸Šå¨±ä¹
    "On.board.service",                   # æœºä¸ŠæœåŠ¡
    "Leg.room.service",                   # è…¿éƒ¨ç©ºé—´æœåŠ¡
    "Baggage.handling",                   # è¡Œæå¤„ç†
    "Checkin.service",                    # å€¼æœºæœåŠ¡
    "Inflight.service",                   # é£è¡Œä¸­æœåŠ¡
    "Cleanliness"                         # æ¸…æ´åº¦
  )
  
  ## æ£€æŸ¥æ‰€æœ‰å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
  ## Check if all required columns exist in the data
  missing_cols <- setdiff(service_cols, names(processed_data))
  if (length(missing_cols) > 0) {
    stop(paste("Missing columns:", paste(missing_cols, collapse = ", ")))
  }
  
  ## ä½¿ç”¨åˆ†å±‚æŠ½æ ·åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ80/20æ¯”ä¾‹ï¼‰
  ## Split data into training and test sets using stratified sampling (80/20 ratio)
  ## createDataPartitionç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„ç±»åˆ«çš„æ¯”ä¾‹ä¸åŸå§‹æ•°æ®ä¸€è‡´
  idx <- caret::createDataPartition(processed_data$satisfaction, p = 0.8, list = FALSE)
  train_df <- processed_data[idx, ]   # è®­ç»ƒé›†ï¼š80%
  test_df  <- processed_data[-idx, ]  # æµ‹è¯•é›†ï¼š20%
  
  message(sprintf("Train set: %d rows, Test set: %d rows", nrow(train_df), nrow(test_df)))
  
  ## åˆ›å»ºç»¼åˆæœåŠ¡åˆ†æ•°ï¼ˆç‰¹å¾å·¥ç¨‹ï¼‰
  ## Create composite service score (feature engineering)
  ## è®¡ç®—æ‰€æœ‰æœåŠ¡è¯„åˆ†çš„å¹³å‡å€¼ï¼Œä½œä¸ºæ•´ä½“æœåŠ¡è´¨é‡çš„ç»¼åˆæŒ‡æ ‡
  train_df$service_score <- rowMeans(train_df[, service_cols], na.rm = TRUE)
  test_df$service_score  <- rowMeans(test_df[, service_cols], na.rm = TRUE)
  
  ## å¯¹åˆ†ç±»å˜é‡è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰
  ## Perform one-hot encoding for categorical variables
  ## é‡è¦ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆç¼–ç å™¨ï¼Œé¿å…æ•°æ®æ³„æ¼ï¼ˆdata leakageï¼‰
  ## å¦‚æœä½¿ç”¨æµ‹è¯•é›†ä¿¡æ¯æ¥æ‹Ÿåˆç¼–ç å™¨ï¼Œä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½è¯„ä¼°è¿‡äºä¹è§‚
  categorical_vars <- c("Gender", "Customer.Type", "Type.of.Travel", "Class")
  dv <- caret::dummyVars(
    ~ Gender + Customer.Type + Type.of.Travel + Class, 
    data = train_df  # åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
  )
  train_dv <- predict(dv, train_df)  # å¯¹è®­ç»ƒé›†ç¼–ç 
  test_dv  <- predict(dv, test_df)    # å¯¹æµ‹è¯•é›†ç¼–ç ï¼ˆä½¿ç”¨è®­ç»ƒé›†æ‹Ÿåˆçš„ç¼–ç å™¨ï¼‰
  
  ## ç»„åˆæ‰€æœ‰ç‰¹å¾ï¼šç‹¬çƒ­ç¼–ç çš„åˆ†ç±»å˜é‡ + æœåŠ¡è¯„åˆ† + ç»¼åˆåˆ†æ•° + æ•°å€¼ç‰¹å¾
  ## Combine all features: one-hot encoded categorical + service ratings + composite score + numeric features
  X_train <- cbind(train_dv, train_df[, c(service_cols, "service_score", 
                                          "Age",                        # å¹´é¾„
                                          "Flight.Distance",            # é£è¡Œè·ç¦»
                                          "Departure.Delay.in.Minutes", # å‡ºå‘å»¶è¯¯ï¼ˆåˆ†é’Ÿï¼‰
                                          "Arrival.Delay.in.Minutes")]) # åˆ°è¾¾å»¶è¯¯ï¼ˆåˆ†é’Ÿï¼‰
  X_test  <- cbind(test_dv, test_df[, c(service_cols, "service_score",
                                        "Age", "Flight.Distance",
                                        "Departure.Delay.in.Minutes",
                                        "Arrival.Delay.in.Minutes")])
  
  ## æå–ç›®æ ‡å˜é‡
  ## Extract target variables
  y_train <- train_df$satisfaction
  y_test  <- test_df$satisfaction
  
  ## è¿”å›æ‰€æœ‰éœ€è¦çš„æ•°æ®å’Œå…ƒä¿¡æ¯
  ## Return all required data and metadata
  return(list(
    X_train = X_train,      # è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µ
    X_test = X_test,        # æµ‹è¯•é›†ç‰¹å¾çŸ©é˜µ
    y_train = y_train,      # è®­ç»ƒé›†æ ‡ç­¾
    y_test = y_test,        # æµ‹è¯•é›†æ ‡ç­¾
    train_df = train_df,    # åŸå§‹è®­ç»ƒæ•°æ®æ¡†ï¼ˆç”¨äºåç»­åˆ†æï¼‰
    test_df = test_df,      # åŸå§‹æµ‹è¯•æ•°æ®æ¡†
    service_cols = service_cols,  # æœåŠ¡åˆ—ååˆ—è¡¨ï¼ˆç”¨äºç‰¹å¾é‡è¦æ€§åˆ†æï¼‰
    positive_class = "satisfied"  # æ­£ç±»æ ‡ç­¾ï¼ˆç”¨äºæ¨¡å‹è¯„ä¼°ï¼‰
  ))
}

## ============================================================================
## 3) æ¨¡å‹è®­ç»ƒå‡½æ•°ï¼ˆå«äº¤å‰éªŒè¯ï¼‰(Model Training with Cross-Validation)
## ============================================================================
## åŠŸèƒ½ï¼šè®­ç»ƒRandom Forestå’ŒXGBoostæ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº¤å‰éªŒè¯è¯„ä¼°
## Purpose: Train RF and XGBoost models, perform cross-validation evaluation

train_models <- function(X_train, y_train, cv_folds = 5) {
  message("Training models with cross-validation...")
  
  ## ========== Random Forest æ¨¡å‹è®­ç»ƒ ==========
  ## ========== Random Forest Model Training ==========
  message("Training Random Forest...")
  rf_model <- randomForest(
    x = X_train,                    # ç‰¹å¾çŸ©é˜µ
    y = y_train,                    # ç›®æ ‡å˜é‡
    ntree = 500,                    # æ ‘çš„æ•°é‡ï¼ˆæ›´å¤šæ ‘é€šå¸¸æ›´å¥½ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ï¼‰
    mtry = sqrt(ncol(X_train)),     # æ¯æ¬¡åˆ†è£‚æ—¶è€ƒè™‘çš„å˜é‡æ•°ï¼ˆsqrtæ˜¯å¸¸ç”¨é€‰æ‹©ï¼‰
    importance = TRUE,              # è®¡ç®—ç‰¹å¾é‡è¦æ€§
    do.trace = FALSE                # ä¸æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
  )
  
  ## ========== XGBoost æ¨¡å‹è®­ç»ƒ ==========
  ## ========== XGBoost Model Training ==========
  message("Training XGBoost...")
  ## å°†å› å­æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼ï¼ˆXGBoostéœ€è¦æ•°å€¼æ ‡ç­¾ï¼‰
  ## Convert factor labels to numeric (XGBoost requires numeric labels)
  y_train_num <- as.integer(y_train == "satisfied")  # satisfied=1, neutral_or_dissatisfied=0
  
  ## åˆ›å»ºXGBoostæ•°æ®çŸ©é˜µ
  ## Create XGBoost data matrix
  dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train_num)
  
  ## è®¾ç½®XGBoostè¶…å‚æ•°
  ## Set XGBoost hyperparameters
  params <- list(
    objective = "binary:logistic",  # äºŒåˆ†ç±»é€»è¾‘å›å½’ç›®æ ‡å‡½æ•°
    eval_metric = "auc",            # è¯„ä¼°æŒ‡æ ‡ï¼šAUCï¼ˆROCæ›²çº¿ä¸‹é¢ç§¯ï¼‰
    max_depth = 6,                  # æ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆæ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼‰
    eta = 0.1,                      # å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼Œè¾ƒå°çš„å€¼éœ€è¦æ›´å¤šè½®æ¬¡ä½†æ›´ç¨³å®šï¼‰
    subsample = 0.8,                # æ¯æ£µæ ‘ä½¿ç”¨çš„æ ·æœ¬æ¯”ä¾‹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    colsample_bytree = 0.8,         # æ¯æ£µæ ‘ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    min_child_weight = 1            # å¶å­èŠ‚ç‚¹æœ€å°æƒé‡ï¼ˆæ§åˆ¶è¿‡æ‹Ÿåˆï¼‰
  )
  
  ## è®­ç»ƒXGBoostæ¨¡å‹
  ## Train XGBoost model
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 200,    # è¿­ä»£è½®æ•°ï¼ˆboostingè½®æ•°ï¼‰
    verbose = 0       # ä¸æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
  )
  
  ## ========== äº¤å‰éªŒè¯ï¼ˆç”¨äºç¨³å¥çš„å‡†ç¡®ç‡è¯„ä¼°ï¼‰==========
  ## ========== Cross-Validation (for robust accuracy assessment) ==========
  message("Performing cross-validation for robust accuracy assessment...")
  
  ## è®¾ç½®äº¤å‰éªŒè¯æ§åˆ¶å‚æ•°
  ## Set cross-validation control parameters
  cv_control <- trainControl(
    method = "cv",                    # äº¤å‰éªŒè¯æ–¹æ³•ï¼škæŠ˜äº¤å‰éªŒè¯
    number = cv_folds,                # æŠ˜æ•°ï¼ˆé»˜è®¤5æŠ˜ï¼‰
    summaryFunction = twoClassSummary, # äºŒåˆ†ç±»æ±‡æ€»å‡½æ•°
    classProbs = TRUE,                # è¿”å›ç±»åˆ«æ¦‚ç‡
    verboseIter = FALSE               # ä¸æ˜¾ç¤ºæ¯æ¬¡è¿­ä»£çš„è¯¦ç»†ä¿¡æ¯
  )
  
  ## Random Forest äº¤å‰éªŒè¯
  ## Random Forest Cross-Validation
  rf_cv <- train(
    x = X_train,
    y = y_train,
    method = "rf",                    # éšæœºæ£®æ—æ–¹æ³•
    trControl = cv_control,
    metric = "ROC",                   # ä¼˜åŒ–æŒ‡æ ‡ï¼šROCï¼ˆAUCï¼‰
    tuneGrid = data.frame(mtry = sqrt(ncol(X_train))),  # å›ºå®šmtryå‚æ•°
    ntree = 500
  )
  
  ## XGBoost äº¤å‰éªŒè¯
  ## XGBoost Cross-Validation
  xgb_cv <- train(
    x = X_train,
    y = y_train,
    method = "xgbTree",              # XGBoostæ ‘æ–¹æ³•
    trControl = cv_control,
    metric = "ROC",
    tuneGrid = expand.grid(           # è¶…å‚æ•°ç½‘æ ¼ï¼ˆè¿™é‡Œä½¿ç”¨å›ºå®šå€¼ï¼‰
      nrounds = 200,
      max_depth = 6,
      eta = 0.1,
      gamma = 0,                      # æœ€å°æŸå¤±å‡å°‘é‡
      colsample_bytree = 0.8,
      min_child_weight = 1,
      subsample = 0.8
    )
  )
  
  ## è¿”å›æ‰€æœ‰æ¨¡å‹å’Œäº¤å‰éªŒè¯ç»“æœ
  ## Return all models and cross-validation results
  return(list(
    rf = rf_model,      # è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹
    xgb = xgb_model,    # è®­ç»ƒå¥½çš„XGBoostæ¨¡å‹
    rf_cv = rf_cv,      # RFäº¤å‰éªŒè¯ç»“æœ
    xgb_cv = xgb_cv     # XGBoostäº¤å‰éªŒè¯ç»“æœ
  ))
}

## ============================================================================
## 4) ç»¼åˆæ¨¡å‹è¯„ä¼°å‡½æ•° (Comprehensive Model Evaluation Function)
## ============================================================================
## åŠŸèƒ½ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œè®¡ç®—å„ç§æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€AUCã€F1ç­‰ï¼‰
## Purpose: Evaluate model performance, calculate various metrics (accuracy, AUC, F1, etc.)

evaluate_models <- function(models, X_test, y_test, positive_class) {
  message("Evaluating models...")
  
  ## ========== æ¨¡å‹é¢„æµ‹ ==========
  ## ========== Model Predictions ==========
  
  ## Random Forest é¢„æµ‹
  ## Random Forest predictions
  rf_pred <- predict(models$rf, X_test)  # é¢„æµ‹ç±»åˆ«
  rf_prob <- predict(models$rf, X_test, type = "prob")[, positive_class]  # é¢„æµ‹æ¦‚ç‡ï¼ˆæ­£ç±»ï¼‰
  
  ## XGBoost é¢„æµ‹
  ## XGBoost predictions
  xgb_prob <- predict(models$xgb, as.matrix(X_test))  # é¢„æµ‹æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰
  ## å°†æ¦‚ç‡è½¬æ¢ä¸ºç±»åˆ«ï¼ˆé˜ˆå€¼0.5ï¼‰
  ## Convert probabilities to classes (threshold 0.5)
  xgb_pred <- factor(
    ifelse(xgb_prob > 0.5, positive_class, "neutral_or_dissatisfied"),
    levels = levels(y_test)
  )
  
  ## ========== æ··æ·†çŸ©é˜µ ==========
  ## ========== Confusion Matrices ==========
  ## æ··æ·†çŸ©é˜µæ˜¾ç¤ºé¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾çš„å¯¹åº”å…³ç³»
  ## Confusion matrix shows correspondence between predictions and true labels
  cm_rf  <- confusionMatrix(rf_pred, y_test, positive = positive_class)
  cm_xgb <- confusionMatrix(xgb_pred, y_test, positive = positive_class)
  
  ## ========== AUCï¼ˆROCæ›²çº¿ä¸‹é¢ç§¯ï¼‰è®¡ç®— ==========
  ## ========== AUC (Area Under ROC Curve) Calculation ==========
  ## AUCè¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿç±»çš„èƒ½åŠ›ï¼Œå€¼è¶Šæ¥è¿‘1è¶Šå¥½
  ## AUC measures model's ability to distinguish positive and negative classes
  y_test_num <- as.integer(y_test == positive_class)  # è½¬æ¢ä¸ºæ•°å€¼ï¼ˆ0/1ï¼‰
  auc_rf  <- as.numeric(pROC::auc(y_test_num, rf_prob))
  auc_xgb <- as.numeric(pROC::auc(y_test_num, xgb_prob))
  
  ## ========== F1åˆ†æ•°è®¡ç®— ==========
  ## ========== F1 Score Calculation ==========
  ## F1 = 2 * (Precision * Recall) / (Precision + Recall)
  ## F1æ˜¯ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ï¼Œå¹³è¡¡ä¸¤è€…
  f1 <- function(cm) {
    pr <- cm$byClass["Precision"]  # ç²¾ç¡®ç‡ï¼šé¢„æµ‹ä¸ºæ­£ç±»ä¸­çœŸæ­£ä¸ºæ­£ç±»çš„æ¯”ä¾‹
    rc <- cm$byClass["Recall"]     # å¬å›ç‡ï¼šçœŸæ­£ä¸ºæ­£ç±»ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
    if (is.na(pr) || is.na(rc) || (pr + rc) == 0) return(NA_real_)
    2 * pr * rc / (pr + rc)
  }
  
  ## ========== ç¼–è¯‘è¯„ä¼°ç»“æœ ==========
  ## ========== Compile Evaluation Results ==========
  results <- data.frame(
    Model = c("Random Forest", "XGBoost"),
    Accuracy = c(cm_rf$overall["Accuracy"], cm_xgb$overall["Accuracy"]),  # å‡†ç¡®ç‡
    Sensitivity = c(cm_rf$byClass["Sensitivity"], cm_xgb$byClass["Sensitivity"]),  # æ•æ„Ÿåº¦ï¼ˆå¬å›ç‡ï¼‰
    Specificity = c(cm_rf$byClass["Specificity"], cm_xgb$byClass["Specificity"]),  # ç‰¹å¼‚åº¦
    Precision = c(cm_rf$byClass["Precision"], cm_xgb$byClass["Precision"]),  # ç²¾ç¡®ç‡
    AUC = c(auc_rf, auc_xgb),  # ROCæ›²çº¿ä¸‹é¢ç§¯
    F1 = c(f1(cm_rf), f1(cm_xgb))  # F1åˆ†æ•°
  )
  
  ## ========== äº¤å‰éªŒè¯ç»“æœæ±‡æ€» ==========
  ## ========== Cross-Validation Results Summary ==========
  ## ä»äº¤å‰éªŒè¯ç»“æœä¸­æå–å‡†ç¡®ç‡å’ŒROCçš„å‡å€¼å’Œæ ‡å‡†å·®
  ## Extract mean and SD of accuracy and ROC from CV results
  rf_acc <- if("Accuracy" %in% names(models$rf_cv$resample)) {
    mean(models$rf_cv$resample$Accuracy, na.rm = TRUE)
  } else NA
  xgb_acc <- if("Accuracy" %in% names(models$xgb_cv$resample)) {
    mean(models$xgb_cv$resample$Accuracy, na.rm = TRUE)
  } else NA
  
  cv_results <- data.frame(
    Model = c("Random Forest (CV)", "XGBoost (CV)"),
    Mean_Accuracy = c(rf_acc, xgb_acc),  # å¹³å‡å‡†ç¡®ç‡
    SD_Accuracy = c(if("Accuracy" %in% names(models$rf_cv$resample)) 
                      sd(models$rf_cv$resample$Accuracy, na.rm = TRUE) else NA,
                    if("Accuracy" %in% names(models$xgb_cv$resample))
                      sd(models$xgb_cv$resample$Accuracy, na.rm = TRUE) else NA),  # å‡†ç¡®ç‡æ ‡å‡†å·®
    Mean_ROC = c(mean(models$rf_cv$resample$ROC, na.rm = TRUE),
                 mean(models$xgb_cv$resample$ROC, na.rm = TRUE)),  # å¹³å‡ROC
    SD_ROC = c(sd(models$rf_cv$resample$ROC, na.rm = TRUE),
               sd(models$xgb_cv$resample$ROC, na.rm = TRUE))  # ROCæ ‡å‡†å·®
  )
  
  ## è¿”å›æ‰€æœ‰è¯„ä¼°ç»“æœ
  ## Return all evaluation results
  return(list(
    results = results,        # æµ‹è¯•é›†è¯„ä¼°ç»“æœ
    cv_results = cv_results,  # äº¤å‰éªŒè¯ç»“æœ
    cm_rf = cm_rf,           # RFæ··æ·†çŸ©é˜µ
    cm_xgb = cm_xgb,         # XGBoostæ··æ·†çŸ©é˜µ
    rf_pred = rf_pred,       # RFé¢„æµ‹ç±»åˆ«
    rf_prob = rf_prob,       # RFé¢„æµ‹æ¦‚ç‡
    xgb_pred = xgb_pred,     # XGBoosté¢„æµ‹ç±»åˆ«
    xgb_prob = xgb_prob,      # XGBoosté¢„æµ‹æ¦‚ç‡
    auc_rf = auc_rf,         # RFçš„AUCå€¼
    auc_xgb = auc_xgb,       # XGBoostçš„AUCå€¼
    y_test_num = y_test_num  # æ•°å€¼åŒ–çš„æµ‹è¯•æ ‡ç­¾
  ))
}

## ============================================================================
## 5) ç‰¹å¾é‡è¦æ€§åˆ†æå‡½æ•° (Feature Importance Analysis Function)
## ============================================================================
## åŠŸèƒ½ï¼šåˆ†æå¹¶æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ï¼Œè¯†åˆ«æœ€é‡è¦çš„é¢„æµ‹å› å­
## Purpose: Analyze and compare feature importance from both models, identify top predictors

analyze_feature_importance <- function(models, X_test, service_cols) {
  message("Analyzing feature importance...")
  
  ## ========== Random Forest ç‰¹å¾é‡è¦æ€§ ==========
  ## ========== Random Forest Feature Importance ==========
  ## RFä½¿ç”¨MeanDecreaseGiniè¡¡é‡ç‰¹å¾é‡è¦æ€§
  ## RF uses MeanDecreaseGini to measure feature importance
  rf_imp <- importance(models$rf)
  rf_imp_df <- data.frame(
    Feature = rownames(rf_imp),
    Importance = rf_imp[, "MeanDecreaseGini"],  # Giniä¸çº¯åº¦å‡å°‘é‡
    Model = "Random Forest"
  )
  rf_imp_df <- rf_imp_df[order(-rf_imp_df$Importance), ]  # æŒ‰é‡è¦æ€§é™åºæ’åˆ—
  
  ## ========== XGBoost ç‰¹å¾é‡è¦æ€§ ==========
  ## ========== XGBoost Feature Importance ==========
  ## XGBoostä½¿ç”¨Gainï¼ˆå¢ç›Šï¼‰è¡¡é‡ç‰¹å¾é‡è¦æ€§
  ## XGBoost uses Gain to measure feature importance
  xgb_imp <- xgb.importance(
    feature_names = colnames(X_test),
    model = models$xgb
  )
  xgb_imp_df <- data.frame(
    Feature = xgb_imp$Feature,
    Importance = xgb_imp$Gain,  # ç‰¹å¾å¸¦æ¥çš„å¢ç›Š
    Model = "XGBoost"
  )
  xgb_imp_df <- xgb_imp_df[order(-xgb_imp_df$Importance), ]  # æŒ‰é‡è¦æ€§é™åºæ’åˆ—
  
  ## ========== å½’ä¸€åŒ–é‡è¦æ€§åˆ†æ•°ï¼ˆ0-100å°ºåº¦ï¼‰ç”¨äºæ¯”è¾ƒ ==========
  ## ========== Normalize Importance Scores (0-100 scale) for Comparison ==========
  ## ç”±äºä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„é‡è¦æ€§åº¦é‡ï¼Œéœ€è¦å½’ä¸€åŒ–ä»¥ä¾¿æ¯”è¾ƒ
  ## Since two models use different importance metrics, normalize for comparison
  rf_imp_df$Importance_Normalized <- (rf_imp_df$Importance / max(rf_imp_df$Importance)) * 100
  xgb_imp_df$Importance_Normalized <- (xgb_imp_df$Importance / max(xgb_imp_df$Importance)) * 100
  
  ## ========== åˆå¹¶ä¸¤ä¸ªæ¨¡å‹çš„é‡è¦æ€§ç»“æœ ==========
  ## ========== Combine Importance from Both Models ==========
  ## åˆå¹¶RFå’ŒXGBoostçš„é‡è¦æ€§åˆ†æ•°ï¼Œè®¡ç®—å¹³å‡å€¼
  ## Merge RF and XGBoost importance scores, calculate average
  combined_imp <- merge(
    rf_imp_df[, c("Feature", "Importance_Normalized")],
    xgb_imp_df[, c("Feature", "Importance_Normalized")],
    by = "Feature",
    suffixes = c("_RF", "_XGB"),
    all = TRUE  # ä¿ç•™æ‰€æœ‰ç‰¹å¾ï¼ˆå³ä½¿æŸä¸ªæ¨¡å‹ä¸­ç¼ºå¤±ï¼‰
  )
  combined_imp[is.na(combined_imp)] <- 0  # ç¼ºå¤±å€¼å¡«å……ä¸º0
  ## è®¡ç®—å¹³å‡é‡è¦æ€§ï¼ˆä¸¤ä¸ªæ¨¡å‹çš„å¹³å‡å€¼ï¼‰
  ## Calculate average importance (mean of both models)
  combined_imp$Average_Importance <- (combined_imp$Importance_Normalized_RF + 
                                       combined_imp$Importance_Normalized_XGB) / 2
  combined_imp <- combined_imp[order(-combined_imp$Average_Importance), ]  # æŒ‰å¹³å‡é‡è¦æ€§æ’åº
  
  ## ========== è¯†åˆ«ç‰¹å¾ç±»å‹ ==========
  ## ========== Identify Feature Types ==========
  ## å°†ç‰¹å¾åˆ†ç±»ä¸ºï¼šæœåŠ¡è¯„åˆ†ã€å®¢æˆ·äººå£ç»Ÿè®¡ã€å…¶ä»–
  ## Categorize features: Service Rating, Customer Demographics, Other
  combined_imp$Feature_Type <- ifelse(
    combined_imp$Feature %in% service_cols | 
    grepl("service", combined_imp$Feature, ignore.case = TRUE),
    "Service Rating",  # æœåŠ¡è¯„åˆ†ç±»ç‰¹å¾
    ifelse(
      grepl("Type|Class|Gender", combined_imp$Feature),
      "Customer Demographics",  # å®¢æˆ·äººå£ç»Ÿè®¡ç‰¹å¾
      "Other"  # å…¶ä»–ç‰¹å¾
    )
  )
  
  ## è¿”å›æ‰€æœ‰é‡è¦æ€§åˆ†æç»“æœ
  ## Return all importance analysis results
  return(list(
    rf_imp_df = rf_imp_df,        # RFç‰¹å¾é‡è¦æ€§æ•°æ®æ¡†
    xgb_imp_df = xgb_imp_df,      # XGBoostç‰¹å¾é‡è¦æ€§æ•°æ®æ¡†
    combined_imp = combined_imp   # åˆå¹¶åçš„ç‰¹å¾é‡è¦æ€§ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
  ))
}

## ============================================================================
## 6) å¯è§†åŒ–å‡½æ•° (Visualization Functions)
## ============================================================================
## åŠŸèƒ½ï¼šåˆ›å»ºå„ç§å¯è§†åŒ–å›¾è¡¨æ¥å±•ç¤ºæ¨¡å‹æ€§èƒ½å’Œç‰¹å¾é‡è¦æ€§
## Purpose: Create various visualizations to showcase model performance and feature importance

create_visualizations <- function(eval_results, importance_results, models, 
                                 X_test, y_test_num, positive_class,
                                 out_dir = "output/zliu134/figures") {
  message("Creating visualizations...")
  
  ## ========== å›¾è¡¨1ï¼šæ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯” ==========
  ## ========== Plot 1: Model Accuracy Comparison ==========
  ## ç›´æ¥å›ç­”ç ”ç©¶é—®é¢˜çš„ç¬¬ä¸€éƒ¨åˆ†ï¼šèƒ½å¦å‡†ç¡®é¢„æµ‹ï¼Ÿ
  ## Directly answers first part of research question: Can we accurately predict?
  p1 <- ggplot(eval_results$results, aes(x = Model, y = Accuracy, fill = Model)) +
    geom_col(alpha = 0.8) +  # æŸ±çŠ¶å›¾
    geom_text(aes(label = sprintf("%.3f", Accuracy)), vjust = -0.5, size = 4) +  # æ·»åŠ æ•°å€¼æ ‡ç­¾
    scale_fill_brewer(palette = "Set2") +  # ä½¿ç”¨é¢œè‰²æ–¹æ¡ˆ
    labs(title = "Model Accuracy Comparison",
         subtitle = "Can we accurately predict passenger satisfaction?",
         y = "Accuracy", x = NULL) +
    theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(size = 14, face = "bold"),
          plot.subtitle = element_text(size = 11))
  
  ## ========== å›¾è¡¨2ï¼šç»¼åˆæ€§èƒ½æŒ‡æ ‡å¯¹æ¯” ==========
  ## ========== Plot 2: Comprehensive Performance Metrics Comparison ==========
  ## å±•ç¤ºå¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼Œå…¨é¢äº†è§£æ¨¡å‹æ€§èƒ½
  ## Show multiple evaluation metrics for comprehensive model understanding
  metrics_long <- eval_results$results %>%
    select(Model, Accuracy, Sensitivity, Specificity, Precision, F1, AUC) %>%
    tidyr::pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")
  
  p2 <- ggplot(metrics_long, aes(x = Metric, y = Value, fill = Model)) +
    geom_col(position = "dodge", alpha = 0.8) +  # å¹¶æ’æŸ±çŠ¶å›¾
    scale_fill_brewer(palette = "Set2") +
    labs(title = "Comprehensive Model Performance Metrics",
         y = "Score", x = "Metric") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),  # xè½´æ ‡ç­¾å€¾æ–œ
          plot.title = element_text(size = 14, face = "bold"))
  
  ## ========== å›¾è¡¨3ï¼šROCæ›²çº¿ ==========
  ## ========== Plot 3: ROC Curves ==========
  ## ROCæ›²çº¿å±•ç¤ºæ¨¡å‹åœ¨ä¸åŒé˜ˆå€¼ä¸‹çš„çœŸé˜³æ€§ç‡å’Œå‡é˜³æ€§ç‡
  ## ROC curve shows true positive rate vs false positive rate at different thresholds
  roc_rf  <- roc(y_test_num, eval_results$rf_prob)
  roc_xgb <- roc(y_test_num, eval_results$xgb_prob)
  
  ## å‡†å¤‡ROCæ•°æ®ç”¨äºç»˜å›¾
  ## Prepare ROC data for plotting
  roc_data <- data.frame(
    FPR = c(1 - roc_rf$specificities, 1 - roc_xgb$specificities),  # å‡é˜³æ€§ç‡
    TPR = c(roc_rf$sensitivities, roc_xgb$sensitivities),          # çœŸé˜³æ€§ç‡
    Model = c(rep("Random Forest", length(roc_rf$sensitivities)),
              rep("XGBoost", length(roc_xgb$sensitivities)))
  )
  
  p3 <- ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
    geom_line(linewidth = 1.2) +  # ROCæ›²çº¿
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +  # å¯¹è§’çº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰
    scale_color_brewer(palette = "Set2") +
    labs(title = sprintf("ROC Curves (AUC: RF=%.3f, XGB=%.3f)",
                         eval_results$auc_rf, eval_results$auc_xgb),
         x = "False Positive Rate", y = "True Positive Rate") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"))
  
  ## ========== å›¾è¡¨4ï¼šæœ€é‡è¦çš„15ä¸ªé¢„æµ‹å› å­ï¼ˆç»¼åˆè§†å›¾ï¼‰==========
  ## ========== Plot 4: Top 15 Most Important Predictors (Combined View) ==========
  ## ç›´æ¥å›ç­”ç ”ç©¶é—®é¢˜çš„ç¬¬äºŒéƒ¨åˆ†ï¼šå“ªäº›å› ç´ æœ€é‡è¦ï¼Ÿ
  ## Directly answers second part of research question: Which factors are most important?
  top_features <- head(importance_results$combined_imp, 15)  # å–å‰15ä¸ª
  top_features$Feature <- factor(top_features$Feature, 
                                 levels = rev(top_features$Feature))  # åè½¬é¡ºåºç”¨äºæ°´å¹³æ¡å½¢å›¾
  
  p4 <- ggplot(top_features, aes(x = Feature, y = Average_Importance, 
                                 fill = Feature_Type)) +
    geom_col(alpha = 0.8) +
    coord_flip() +  # æ°´å¹³æ¡å½¢å›¾
    scale_fill_brewer(palette = "Set1") +
    labs(title = "Top 15 Most Important Predictors (Average of Both Models)",
         subtitle = "Which factors are the most important predictors?",
         x = NULL, y = "Average Normalized Importance",
         fill = "Feature Type") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"),
          plot.subtitle = element_text(size = 11))
  
  ## ========== å›¾è¡¨5ï¼šç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆRF vs XGBoostï¼‰==========
  ## ========== Plot 5: Feature Importance Comparison (RF vs XGBoost) ==========
  ## å±•ç¤ºä¸¤ä¸ªæ¨¡å‹å¯¹åŒä¸€ç‰¹å¾çš„é‡è¦æ€§è¯„ä¼°æ˜¯å¦ä¸€è‡´
  ## Show if both models agree on feature importance
  top_15_rf <- head(importance_results$rf_imp_df, 15)
  top_15_xgb <- head(importance_results$xgb_imp_df, 15)
  
  ## æ‰¾å‡ºä¸¤ä¸ªæ¨¡å‹å…±åŒè®¤ä¸ºé‡è¦çš„ç‰¹å¾
  ## Find features that both models consider important
  common_features <- intersect(top_15_rf$Feature, top_15_xgb$Feature)
  comparison_data <- data.frame(
    Feature = common_features,
    RF_Importance = top_15_rf$Importance_Normalized[match(common_features, top_15_rf$Feature)],
    XGB_Importance = top_15_xgb$Importance_Normalized[match(common_features, top_15_xgb$Feature)]
  )
  comparison_data <- comparison_data[order(-(comparison_data$RF_Importance + 
                                              comparison_data$XGB_Importance)), ]
  comparison_data$Feature <- factor(comparison_data$Feature, 
                                    levels = rev(comparison_data$Feature))
  
  ## è½¬æ¢ä¸ºé•¿æ ¼å¼ç”¨äºç»˜å›¾
  ## Convert to long format for plotting
  comparison_long <- comparison_data %>%
    tidyr::pivot_longer(cols = c(RF_Importance, XGB_Importance),
                       names_to = "Model", values_to = "Importance")
  comparison_long$Model <- gsub("_Importance", "", comparison_long$Model)
  
  p5 <- ggplot(comparison_long, aes(x = Feature, y = Importance, fill = Model)) +
    geom_col(position = "dodge", alpha = 0.8) +  # å¹¶æ’æŸ±çŠ¶å›¾
    coord_flip() +
    scale_fill_brewer(palette = "Set2") +
    labs(title = "Feature Importance: RF vs XGBoost Comparison",
         subtitle = "Top features identified by both models",
         x = NULL, y = "Normalized Importance") +
    theme_minimal() +
    theme(plot.title = element_text(size = 14, face = "bold"))
  
  ## ========== å›¾è¡¨6ï¼šæ··æ·†çŸ©é˜µ ==========
  ## ========== Plot 6: Confusion Matrices ==========
  ## æ··æ·†çŸ©é˜µæ˜¾ç¤ºæ¨¡å‹çš„åˆ†ç±»é”™è¯¯æ¨¡å¼
  ## Confusion matrix shows model's classification error patterns
  cm_rf_data <- as.data.frame(eval_results$cm_rf$table)
  cm_xgb_data <- as.data.frame(eval_results$cm_xgb$table)
  
  ## Random Forestæ··æ·†çŸ©é˜µ
  p6a <- ggplot(cm_rf_data, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white") +  # çƒ­åŠ›å›¾
    geom_text(aes(label = Freq), color = "black", size = 5) +  # æ·»åŠ æ•°å€¼
    scale_fill_gradient(low = "white", high = "steelblue") +  # é¢œè‰²æ¸å˜
    labs(title = "Random Forest Confusion Matrix",
         x = "Actual", y = "Predicted") +
    theme_minimal()
  
  ## XGBoostæ··æ·†çŸ©é˜µ
  p6b <- ggplot(cm_xgb_data, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white") +
    geom_text(aes(label = Freq), color = "black", size = 5) +
    scale_fill_gradient(low = "white", high = "steelblue") +
    labs(title = "XGBoost Confusion Matrix",
         x = "Actual", y = "Predicted") +
    theme_minimal()
  
  ## ========== ä¿å­˜æ‰€æœ‰å›¾è¡¨ ==========
  ## ========== Save All Plots ==========
  ggsave(file.path(out_dir, "01_accuracy_comparison.png"), p1, 
         width = 8, height = 6, dpi = 300)
  ggsave(file.path(out_dir, "02_comprehensive_metrics.png"), p2, 
         width = 10, height = 6, dpi = 300)
  ggsave(file.path(out_dir, "03_roc_curves.png"), p3, 
         width = 8, height = 6, dpi = 300)
  ggsave(file.path(out_dir, "04_top_features_combined.png"), p4, 
         width = 10, height = 7, dpi = 300)
  ggsave(file.path(out_dir, "05_feature_importance_comparison.png"), p5, 
         width = 10, height = 7, dpi = 300)
  ggsave(file.path(out_dir, "06a_confusion_matrix_rf.png"), p6a, 
         width = 6, height = 5, dpi = 300)
  ggsave(file.path(out_dir, "06b_confusion_matrix_xgb.png"), p6b, 
         width = 6, height = 5, dpi = 300)
  
  ## ========== å›¾è¡¨7ï¼šåä¾èµ–å›¾ï¼ˆPDPï¼‰==========
  ## ========== Plot 7: Partial Dependence Plot (PDP) ==========
  ## å±•ç¤ºæœ€é‡è¦ç‰¹å¾å¦‚ä½•å½±å“æ»¡æ„åº¦é¢„æµ‹æ¦‚ç‡
  ## Show how the most important feature affects satisfaction prediction probability
  top_feature <- importance_results$combined_imp$Feature[1]  # æœ€é‡è¦çš„ç‰¹å¾
  if (top_feature %in% colnames(X_test)) {
    tryCatch({
      ## è®¡ç®—åä¾èµ–ï¼šå›ºå®šå…¶ä»–ç‰¹å¾ï¼Œåªæ”¹å˜ç›®æ ‡ç‰¹å¾ï¼Œè§‚å¯Ÿé¢„æµ‹æ¦‚ç‡çš„å˜åŒ–
      ## Calculate partial dependence: fix other features, vary target feature, observe prediction change
      pd <- partial(models$xgb, pred.var = top_feature,
                   train = as.matrix(X_test), which.class = 1, prob = TRUE)
      p7 <- autoplot(pd) + 
        ggtitle(sprintf("Partial Dependence: %s", top_feature),
                subtitle = "How this top predictor affects satisfaction probability") +
        theme_minimal() +
        theme(plot.title = element_text(size = 14, face = "bold"))
      ggsave(file.path(out_dir, sprintf("07_pdp_%s.png", 
                                        gsub("[^A-Za-z0-9_]", "_", top_feature))),
             p7, width = 8, height = 6, dpi = 300)
    }, error = function(e) {
      message(sprintf("Could not create PDP for %s: %s", top_feature, e$message))
    })
  }
  
  message("All visualizations saved to ", out_dir)
  
  ## è¿”å›æ‰€æœ‰å›¾è¡¨å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºåç»­ç»„åˆæˆ–ä¿®æ”¹ï¼‰
  ## Return all plot objects (optional, for later combination or modification)
  return(list(
    p1 = p1, p2 = p2, p3 = p3, p4 = p4, p5 = p5, p6a = p6a, p6b = p6b
  ))
}

## ============================================================================
## 7) ä¸»æ‰§è¡Œå‡½æ•° (Main Execution Function)
## ============================================================================
## åŠŸèƒ½ï¼šæ•´åˆæ‰€æœ‰æ­¥éª¤ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
## Purpose: Integrate all steps, execute complete analysis pipeline

run_all <- function() {
  ## æ‰“å°åˆ†æå¼€å§‹ä¿¡æ¯
  ## Print analysis start information
  message("\n==========================================")
  message("Passenger Satisfaction Prediction Analysis")
  message("RQ: Can we accurately predict passenger satisfaction,")
  message("     and which factors are the most important predictors?")
  message("==========================================\n")
  
  ## ========== æ­¥éª¤1ï¼šåŠ è½½æ•°æ® ==========
  ## ========== Step 1: Load Data ==========
  message(">>> Step 1: Loading data...")
  if (!file.exists("dataset/train.csv")) {
    stop("dataset/train.csv not found!")
  }
  train_data <- read.csv("dataset/train.csv")
  message(sprintf("Loaded %d rows, %d columns", nrow(train_data), ncol(train_data)))
  
  ## ========== æ­¥éª¤2ï¼šæ•°æ®é¢„å¤„ç† ==========
  ## ========== Step 2: Data Preprocessing ==========
  message("\n>>> Step 2: Preprocessing data...")
  processed <- preprocess_data(train_data)
  
  ## ========== æ­¥éª¤3ï¼šç‰¹å¾å·¥ç¨‹å’Œæ•°æ®åˆ’åˆ† ==========
  ## ========== Step 3: Feature Engineering and Data Split ==========
  message("\n>>> Step 3: Splitting data and engineering features...")
  se <- split_and_engineer(processed)
  
  ## ========== æ­¥éª¤4ï¼šæ¨¡å‹è®­ç»ƒ ==========
  ## ========== Step 4: Model Training ==========
  message("\n>>> Step 4: Training models...")
  models <- train_models(se$X_train, se$y_train)
  
  ## ========== æ­¥éª¤5ï¼šæ¨¡å‹è¯„ä¼° ==========
  ## ========== Step 5: Model Evaluation ==========
  message("\n>>> Step 5: Evaluating models...")
  eval_results <- evaluate_models(
    models, se$X_test, se$y_test, se$positive_class
  )
  
  ## æ‰“å°è¯„ä¼°ç»“æœ
  ## Print evaluation results
  message("\n>>> Model Performance Results:")
  print(eval_results$results)
  message("\n>>> Cross-Validation Results:")
  print(eval_results$cv_results)
  
  ## ========== æ­¥éª¤6ï¼šç‰¹å¾é‡è¦æ€§åˆ†æ ==========
  ## ========== Step 6: Feature Importance Analysis ==========
  message("\n>>> Step 6: Analyzing feature importance...")
  importance_results <- analyze_feature_importance(
    models, se$X_test, se$service_cols
  )
  
  ## æ‰“å°æœ€é‡è¦çš„10ä¸ªç‰¹å¾
  ## Print top 10 most important features
  message("\n>>> Top 10 Most Important Features:")
  print(head(importance_results$combined_imp[, c("Feature", "Average_Importance", 
                                                   "Feature_Type")], 10))
  
  ## ========== æ­¥éª¤7ï¼šåˆ›å»ºå¯è§†åŒ– ==========
  ## ========== Step 7: Create Visualizations ==========
  message("\n>>> Step 7: Creating visualizations...")
  plots <- create_visualizations(
    eval_results, importance_results, models,
    se$X_test, eval_results$y_test_num, se$positive_class
  )
  
  ## ========== æ­¥éª¤8ï¼šä¿å­˜ç»“æœ ==========
  ## ========== Step 8: Save Results ==========
  message("\n>>> Step 8: Saving results...")
  ## ä¿å­˜æ¨¡å‹è¯„ä¼°ç»“æœ
  ## Save model evaluation results
  write.csv(eval_results$results, 
            "output/zliu134/models/model_evaluation.csv", row.names = FALSE)
  ## ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
  ## Save cross-validation results
  write.csv(eval_results$cv_results, 
            "output/zliu134/models/cv_results.csv", row.names = FALSE)
  ## ä¿å­˜ç‰¹å¾é‡è¦æ€§ç»“æœ
  ## Save feature importance results
  write.csv(importance_results$rf_imp_df, 
            "output/zliu134/models/rf_feature_importance.csv", row.names = FALSE)
  write.csv(importance_results$xgb_imp_df, 
            "output/zliu134/models/xgb_feature_importance.csv", row.names = FALSE)
  write.csv(importance_results$combined_imp, 
            "output/zliu134/models/combined_feature_importance.csv", row.names = FALSE)
  
  ## ========== åˆ†ææ€»ç»“ ==========
  ## ========== Analysis Summary ==========
  message("\n==========================================")
  message("âœ… Analysis Complete!")
  message("==========================================")
  message("\nğŸ“Š Key Findings:")
  ## æœ€ä½³æ¨¡å‹å‡†ç¡®ç‡
  ## Best model accuracy
  message(sprintf("   â€¢ Best Model Accuracy: %.3f (%s)",
                  max(eval_results$results$Accuracy),
                  eval_results$results$Model[which.max(eval_results$results$Accuracy)]))
  ## æœ€ä½³æ¨¡å‹AUC
  ## Best model AUC
  message(sprintf("   â€¢ Best Model AUC: %.3f (%s)",
                  max(eval_results$results$AUC),
                  eval_results$results$Model[which.max(eval_results$results$AUC)]))
  ## æœ€é‡è¦çš„é¢„æµ‹å› å­
  ## Top predictor
  message(sprintf("   â€¢ Top Predictor: %s (Importance: %.2f)",
                  importance_results$combined_imp$Feature[1],
                  importance_results$combined_imp$Average_Importance[1]))
  message("\nğŸ“ Output Files:")
  message("   â€¢ CSV files: output/zliu134/models/")
  message("   â€¢ Figures: output/zliu134/figures/")
  message("==========================================\n")
  
  ## è¿”å›æ‰€æœ‰ç»“æœå¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºåç»­åˆ†æï¼‰
  ## Return all result objects (optional, for further analysis)
  return(list(
    models = models,                    # è®­ç»ƒå¥½çš„æ¨¡å‹
    eval_results = eval_results,        # è¯„ä¼°ç»“æœ
    importance_results = importance_results,  # ç‰¹å¾é‡è¦æ€§ç»“æœ
    plots = plots                       # å›¾è¡¨å¯¹è±¡
  ))
}

## ============================================================================
## 8) æ‰§è¡Œåˆ†æ (Execute Analysis)
## ============================================================================
## å¦‚æœè„šæœ¬ä¸æ˜¯ä»¥äº¤äº’æ¨¡å¼è¿è¡Œï¼ˆå³ç›´æ¥è¿è¡Œè„šæœ¬ï¼‰ï¼Œåˆ™æ‰§è¡Œå®Œæ•´åˆ†æ
## If script is not run in interactive mode (i.e., run directly), execute full analysis
if (!interactive()) {
  results <- run_all()
}
